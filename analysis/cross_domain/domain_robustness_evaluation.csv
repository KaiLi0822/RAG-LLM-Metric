Metric,Model,techqa_rewrite_PassRate,emanual_rewrite_PassRate,delucionqa_rewrite_PassRate,rewrite_median(PassRate),rewrite_MAD,rewrite_RMAD,techqa_wrong_PassRate,emanual_wrong_PassRate,delucionqa_wrong_PassRate,wrong_median(PassRate),wrong_MAD,wrong_RMAD,techqa_ground_truth_PassRate,emanual_ground_truth_PassRate,delucionqa_ground_truth_PassRate,ground_truth_median(PassRate),ground_truth_MAD,ground_truth_RMAD
Adherence_Faithfulness_faithfulness_score,DeepSeek7b,0.71,0.795,0.77,0.77,0.025000000000000022,0.03246753246753249,0.6225,0.56,0.49,0.56,0.0625,0.11160714285714285,0.71,0.83,0.8225,0.8225,0.007499999999999951,0.009118541033434591
COHERENCE_coherence_score,DeepSeek7b,0.91,0.9375,0.9225,0.9225,0.012499999999999956,0.013550135501354966,0.81,0.835,0.85,0.835,0.015000000000000013,0.017964071856287442,0.93,0.945,0.95,0.945,0.0050000000000000044,0.005291005291005296
Context_Relevance_relevance_score,DeepSeek7b,0.5625,0.6425,0.6425,0.6425,0.0,0.0,0.5925,0.6175,0.655,0.6175,0.025000000000000022,0.040485829959514205,0.5575,0.605,0.695,0.605,0.04749999999999999,0.07851239669421485
Context_Utilization_context_utilization_score,DeepSeek7b,0.5325,0.4875,0.45,0.4875,0.03749999999999998,0.07692307692307689,0.5625,0.4725,0.41,0.4725,0.0625,0.1322751322751323,0.515,0.4425,0.4625,0.4625,0.020000000000000018,0.04324324324324328
FACTUAL_ACCURACY_accuracy_score,DeepSeek7b,0.8775,0.9025,0.8975,0.8975,0.0050000000000000044,0.005571030640668529,0.85,0.7325,0.685,0.7325,0.04749999999999999,0.06484641638225254,0.8725,0.87,0.8925,0.8725,0.0025000000000000577,0.0028653295128940486
answer_equivalence_equivalence_score,DeepSeek7b,0.095,0.2175,0.22,0.2175,0.0025000000000000022,0.011494252873563229,0.0925,0.1,0.0875,0.0925,0.0050000000000000044,0.054054054054054106,0.125,0.2,0.2375,0.2,0.03749999999999998,0.1874999999999999
answer_similarity,DeepSeek7b,0.9925,0.995,0.9775,0.9925,0.0024999999999999467,0.002518891687657377,0.99,0.98,0.9675,0.98,0.010000000000000009,0.01020408163265307,1.0,1.0,1.0,1.0,0.0,0.0
engagement_engagement_score,DeepSeek7b,0.9625,0.715,0.74,0.74,0.025000000000000022,0.03378378378378381,0.985,0.64,0.7175,0.7175,0.07750000000000001,0.10801393728222998,0.965,0.655,0.74,0.74,0.08499999999999996,0.11486486486486482
factual_correctness_F1_score,DeepSeek7b,0.825,0.9075,0.8675,0.8675,0.039999999999999925,0.046109510086455245,0.31,0.2,0.2075,0.2075,0.007499999999999979,0.03614457831325291,0.93,0.9575,0.96,0.9575,0.0024999999999999467,0.0026109660574411974
key_point_completeness_score,DeepSeek7b,0.8325,0.8875,0.855,0.855,0.022499999999999964,0.02631578947368417,0.6425,0.59,0.5075,0.59,0.05249999999999999,0.08898305084745761,0.825,0.8475,0.8525,0.8475,0.0050000000000000044,0.005899705014749268
key_point_hallucination_score,DeepSeek7b,0.8575,0.92,0.92,0.92,0.0,0.0,0.89,0.8575,0.785,0.8575,0.03249999999999997,0.03790087463556848,0.9025,0.93,0.9325,0.93,0.0024999999999999467,0.0026881720430106952
key_point_irrelevant_score,DeepSeek7b,0.795,0.865,0.835,0.835,0.030000000000000027,0.035928143712574884,0.7825,0.7975,0.75,0.7825,0.015000000000000013,0.019169329073482445,0.835,0.8725,0.825,0.835,0.010000000000000009,0.011976047904191628
learning_facilitation_learning_facilitation_score,DeepSeek7b,0.9625,0.9425,0.945,0.945,0.0024999999999999467,0.0026455026455025894,0.975,0.8925,0.8625,0.8925,0.029999999999999916,0.03361344537815117,0.97,0.96,0.955,0.96,0.0050000000000000044,0.005208333333333338
refusal_accuracy_refusal_accuracy,DeepSeek7b,0.4,0.2475,0.225,0.2475,0.022499999999999992,0.09090909090909088,0.395,0.2625,0.285,0.285,0.022499999999999964,0.07894736842105252,0.4325,0.25,0.27,0.27,0.020000000000000018,0.07407407407407414
