{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T17:58:05.090903Z",
     "start_time": "2025-02-16T17:58:00.242347Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import login, HfApi\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "NEW_DATASET_NAME = \"RAGEVALUATION-HJKMY/ragbench_emanual_400row\"  \n",
    "SPLITS_TO_SAMPLE = ['train']\n",
    "\n",
    "original_dataset = load_dataset(\"rungalileo/ragbench\", \"emanual\")\n",
    "\n",
    "new_dataset_dict = {}\n",
    "for split in SPLITS_TO_SAMPLE:\n",
    "    if split in original_dataset:\n",
    "        filtered_split = original_dataset[split].filter(lambda x: x['adherence_score'] == True)\n",
    "        sampled_split = filtered_split.select(range(min(400, len(filtered_split))))  # Ensure there are enough rows\n",
    "        new_dataset_dict[split] = sampled_split\n",
    "\n",
    "new_dataset = DatasetDict(new_dataset_dict)\n",
    "\n",
    "\n",
    "login(token=os.getenv('HF_TOKEN'))\n",
    "\n",
    "new_dataset.push_to_hub(NEW_DATASET_NAME)\n",
    "\n",
    "print(f\"Dataset uploaded successfully to {NEW_DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e812d6f5e14e343a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T17:56:46.404166Z",
     "start_time": "2025-02-16T17:56:46.401888Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'delu_Retreival Setting', 'delu_Question', 'delu_Context', 'delu_Answer', 'delu_Answer_sent_tokenized', 'delu_Sentence_labels', 'delu_Label', 'delu_Answerable', 'delu_Does_not_answer', 'question', 'documents', 'response', 'documents_sentences', 'response_sentences', 'sentence_support_information', 'unsupported_response_sentence_keys', 'overall_supported_explanation', 'relevance_explanation', 'all_relevant_sentence_keys', 'all_utilized_sentence_keys', 'relevance_score', 'utilization_score', 'completeness_score'],\n",
      "    num_rows: 400\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 61.71ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded successfully to RAGEVALUATION-HJKMY/ragbench_delucionqa_400row\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "NEW_DATASET_NAME = \"RAGEVALUATION-HJKMY/ragbench_delucionqa_400row\"  \n",
    "SPLITS_TO_SAMPLE = 'train'\n",
    "# load local train.csv\n",
    "# the first title line: sample_id,Retreival Setting,Question,Context,Answer,Answer_sent_tokenized,Sentence_labels,Label,Answerable,Does_not_answer\n",
    "local_data = pd.read_csv(\"/Users/likai/IdeaProjects/RAG-LLM-Metric/personal/kai/train.csv\")\n",
    "# keep data that Label = Not Hallucinated and Answerable = TRUE\n",
    "local_filtered = local_data[(local_data['Label'] == 'Not Hallucinated') & (local_data['Answerable'] == True)]\n",
    "local_filtered = local_filtered.add_prefix(\"delu_\")\n",
    "# local data from ragbench\n",
    "ragbench_dataset = load_dataset(\"rungalileo/ragbench\", \"delucionqa\")\n",
    "filtered_rag = ragbench_dataset[SPLITS_TO_SAMPLE].filter(lambda x: x['adherence_score'] == True)\n",
    "rag_df = filtered_rag.to_pandas()\n",
    "rag_df = rag_df[['question', 'documents','response','documents_sentences','response_sentences','sentence_support_information','unsupported_response_sentence_keys',\n",
    "                 'overall_supported_explanation','relevance_explanation','all_relevant_sentence_keys',\n",
    "                 'all_utilized_sentence_keys','relevance_score','utilization_score','completeness_score']]\n",
    "# keep and deduplicate the question and documents column of filtered_rag\n",
    "rag_df = rag_df.drop_duplicates(['question'])\n",
    "\n",
    "# left join the local data with the filtered_rag where filtered_rag['question'] = local['question']\n",
    "merged_data = local_filtered.merge(rag_df, left_on=\"delu_Question\", right_on=\"question\")\n",
    "merged_data.rename(columns={'delu_sample_id': 'id'}, inplace=True)\n",
    "# keep 400 rows data\n",
    "final_dataset = Dataset.from_pandas(merged_data.iloc[:400])\n",
    "\n",
    "print(final_dataset)\n",
    "\n",
    "new_dataset_dict[SPLITS_TO_SAMPLE] = final_dataset\n",
    "\n",
    "new_dataset = DatasetDict(new_dataset_dict)\n",
    "\n",
    "\n",
    "login(token=os.getenv('HF_TOKEN'))\n",
    "\n",
    "new_dataset.push_to_hub(NEW_DATASET_NAME)\n",
    "\n",
    "print(f\"Dataset uploaded successfully to {NEW_DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7311b03f4a867",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-llm-metric-GKTD7G0x-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
