{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict\n",
    "import json\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from evaluator.base_evaluator import RAGEvaluator\n",
    "from evaluator.llm import OpenAIClientLLM\n",
    "from evaluator.prompt_manager import EvaluationType, PromptManager\n",
    "\n",
    "class LLMAsJudgeEvaluator(RAGEvaluator):\n",
    "    def pre_process(\n",
    "        self,\n",
    "        answer: str,\n",
    "        **kwargs\n",
    "    ) -> str:\n",
    "        question = kwargs.get(\"question\", \"\")\n",
    "        context = kwargs.get(\"context\", \"\")\n",
    "        return self.prompt_manager.build_prompt(\n",
    "            answer=answer,\n",
    "            question=question,\n",
    "            context=context,\n",
    "            eval_type=EvaluationType.RELEVANCE  # or make this configurable\n",
    "        )\n",
    "    \n",
    "    def call_llm(self, processed_data: str) -> str:\n",
    "        # Execute LLM call with constructed prompt\n",
    "        return self.llm.generate(processed_data)\n",
    "    \n",
    "    def post_process(self, llm_response: str) -> Dict[str, float]:\n",
    "        \"\"\"Parse JSON response into scores dictionary\"\"\"\n",
    "        try:\n",
    "            # Clean response and parse JSON\n",
    "            response_text = llm_response.strip().replace('```json', '').replace('```', '')\n",
    "            result = json.loads(response_text)\n",
    "            \n",
    "            # Normalize scores and flatten structure\n",
    "            scores = {\n",
    "                'score': result.get('score', \n",
    "                           result.get('relevance_score', \n",
    "                           result.get('coherence_score', \n",
    "                           result.get('accuracy_score', 0.0)))),\n",
    "                'confidence': result.get('confidence', 0.0)\n",
    "            }\n",
    "            \n",
    "            # Add additional metrics\n",
    "            for key in result:\n",
    "                if key.endswith('_score') and key != 'score':\n",
    "                    scores[key] = result[key]\n",
    "            \n",
    "            return scores\n",
    "            \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error parsing LLM response: {e}\")\n",
    "            return {\n",
    "                'score': 0.0,\n",
    "                'confidence': 0.0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "delucionqa = load_dataset(\"rungalileo/ragbench\", \"delucionqa\")\n",
    "df = delucionqa['train'].to_pandas()\n",
    "a = df.head()\n",
    "a['flatten_doc'] = a.apply(lambda x: \"\\n\".join([f\"`{label}` {sentence}\" for label, sentence in [inner_list for middle_list in x['documents_sentences'] for inner_list in middle_list]]), axis = 1)\n",
    "answer = a.iloc[1]['response']\n",
    "documents = a.iloc[1]['flatten_doc']\n",
    "question = a.iloc[1]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer, \"\\n\\n\")\n",
    "print(documents,  \"\\n\\n\")\n",
    "print(question, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Openai key is the CentML key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"vjn61Mx-WYYUB07Jez2kRj41k0TIZsyt52M0RaM0Chg\"\n",
    "\n",
    "evaluator = LLMAsJudgeEvaluator(\n",
    "    llm=OpenAIClientLLM(),\n",
    "    prompt_manager=PromptManager(default_type=EvaluationType.FACTUAL_ACCURACY)\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(\n",
    "    answer=answer,\n",
    "    question=question,\n",
    "    context=documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Factual Correctness\n",
    "This is the F1-Score of statements in RAG answer classified as True Positive, False Positive and False Negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Implement an evaluator for Factual Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Dict\n",
    "import json\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from evaluator.base_evaluator import RAGEvaluator\n",
    "from evaluator.llm import OpenAIClientLLM\n",
    "from evaluator.prompt_manager import EvaluationType, PromptManager\n",
    "\n",
    "class FacCorEvaluator(RAGEvaluator):\n",
    "    def pre_process(\n",
    "        self,\n",
    "        answer: str,\n",
    "        golden_answer: str\n",
    "    ) -> str:\n",
    "        return self.prompt_manager.build_prompt(\n",
    "            answer=answer,\n",
    "            golden_answer = golden_answer\n",
    "        )\n",
    "    def call_llm(self, processed_data: str) -> str:\n",
    "        # Execute LLM call with constructed prompt\n",
    "        return self.llm.generate(processed_data)\n",
    "    \n",
    "    def post_process(self, llm_response: str) -> Dict[str, float]:\n",
    "        \"\"\"Parse JSON response into scores dictionary\"\"\"\n",
    "        return llm_response\n",
    "        # try:\n",
    "        #     # Clean response and parse JSON\n",
    "        #     response_text = llm_response.strip().replace('```json', '').replace('```', '')\n",
    "        #     result = json.loads(response_text)\n",
    "            \n",
    "        #     # Normalize scores and flatten structure\n",
    "        #     scores = {\n",
    "        #         'score': result.get('score', \n",
    "        #                    result.get('relevance_score', \n",
    "        #                    result.get('coherence_score', \n",
    "        #                    result.get('accuracy_score', 0.0)))),\n",
    "        #         'confidence': result.get('confidence', 0.0)\n",
    "        #     }\n",
    "            \n",
    "        #     # Add additional metrics\n",
    "        #     for key in result:\n",
    "        #         if key.endswith('_score') and key != 'score':\n",
    "        #             scores[key] = result[key]\n",
    "            \n",
    "        #     return scores\n",
    "            \n",
    "        # except (json.JSONDecodeError, KeyError) as e:\n",
    "        #     print(f\"Error parsing LLM response: {e}\")\n",
    "        #     return {\n",
    "        #         'score': 0.0,\n",
    "        #         'confidence': 0.0,\n",
    "        #         'error': str(e)\n",
    "        #     }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Test with a test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Openai key is the CentML key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"vjn61Mx-WYYUB07Jez2kRj41k0TIZsyt52M0RaM0Chg\"\n",
    "\n",
    "evaluator = FacCorEvaluator(\n",
    "    llm=OpenAIClientLLM(),\n",
    "    prompt_manager=PromptManager(default_type=EvaluationType.FACTUAL_CORRECTNESS)\n",
    ")\n",
    "\n",
    "\n",
    "result = evaluator.evaluate(\n",
    "    answer=\"The Great Wall of China is located in southern China. It was built to protect against Mongolian invasions and stretches over 15,000 miles.\",\n",
    "    golden_answer=\"The Great Wall of China is located in northern China. It was originally built to protect against invasions and raids from nomadic groups and stretches over 13,000 miles.\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"extracted_statements\": {\\n    \"golden\": [\"The Great Wall of China is located in northern China\", \"It was originally built to protect against invasions and raids from nomadic groups\", \"It stretches over 13,000 miles\"],\\n    \"generated\": [\"The Great Wall of China is located in southern China\", \"It was built to protect against Mongolian invasions\", \"It stretches over 15,000 miles\"]\\n  },\\n  \"TP\": 0,\\n  \"FP\": 3,\\n  \"FN\": 3,\\n  \"factual_correctness_score\": 0.0,\\n  \"reasons\": [\"Incorrect location mentioned\", \"Incorrect purpose and invader details\", \"Incorrect length\"]\\n}\\n```'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
