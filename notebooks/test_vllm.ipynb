{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/work/van-speech-nlp/jindaznb/mmenv/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "0.16.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "logger.info(torch.__version__)\n",
    "logger.info(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 01:13:32,671\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.6.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import Dict\n",
    "import json\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from evaluator.base_evaluator import RAGEvaluator\n",
    "from utils.llm import HFClient, HFClientVLLM\n",
    "from evaluator.prompt_manager import EvaluationType, PromptManager\n",
    "from evaluator.evaluators import LearningFacilitationEvaluator, EngagementEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "delucionqa = load_dataset(\"rungalileo/ragbench\", \"delucionqa\")\n",
    "df = delucionqa['train'].to_pandas()\n",
    "a = df.head()\n",
    "# a['flatten_doc'] = a.apply(lambda x: \"\\n\".join([f\"`{label}` {sentence}\" for label, sentence in [inner_list for middle_list in x['documents_sentences'] for inner_list in middle_list]]), axis = 1)\n",
    "answer = a.iloc[1]['response']\n",
    "documents = a.iloc[1]['documents']\n",
    "question = a.iloc[1]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To customize the Uconnect system based on your own preferences, you can follow these steps:\n",
      "\n",
      "1. Press the apps button on the touchscreen to open the app screen.\n",
      "2. Press and hold the selected app, then drag it to replace an existing shortcut in the main menu bar.\n",
      "3. Note that this feature is only available when the vehicle is in PARK. \n",
      "\n",
      "\n",
      "[' Uconnect 4 with 7-inch display and uconnect 4/4c/4c nav with 8.4-inch display press the apps button, then press the settings button on the touchscreen to display the menu setting screen.  In this mode the Uconnect system allows you to access programmable features.  When making a selection, only press one button at a time to enter the desired menu.  Once in the desired menu, press and release the preferred setting option until a check mark appears next to the setting, showing that setting has been selected.  Once the setting is complete, press the X button on the touchscreen to close out of the settings screen.  Pressing the Up or Down Arrow button on the right side of the screen will allow you to toggle up or down through the available settings.  Note: Depending on the vehicle’s options, feature settings may vary.  All settings should be changed with the ignition in the ON/RUN position.'\n",
      " ' Drag & Drop Menu Bar The Uconnect features and services in the main menu bar are easily customized for your preference.  Simply follow these steps: press the apps button to open the app screen.  Press and hold, then drag the selected app to replace an existing shortcut in the main menu bar.  Note: This feature is only available if the vehicle is in PARK.  Uconnect 4 with 7-inch display and uconnect 4/4c/4c nav with 8.4-inch display press the apps button, then press the settings button on the touchscreen to display the menu setting screen.  In this mode the Uconnect system allows you to access programmable features.  When making a selection, only press one button at a time to enter the desired menu.  Once in the desired menu, press and release the preferred setting option until a check mark appears next to the setting, showing that setting has been selected.  Once the setting is complete, press the X button on the touchscreen to close out of the settings screen.  Pressing the Up or Down Arrow button on the right side of the screen will allow you to toggle up or down through the available settings.  Note: Depending on the vehicle’s options, feature settings may vary.  All settings should be changed with the ignition in the ON/RUN position.  UCONNECT SETTINGS The Uconnect system uses a combination of buttons on the touchscreen and buttons on the faceplate located on the center of the instrument panel.  These buttons allow you to access and change the Customer Programmable Features.  Many features can vary by vehicle.  Buttons on the faceplate are located below and/or beside the Uconnect system in the center of the instrument panel.  In addition, there is a SCROLL/ENTER control knob located on the right side.  Turn the control knob to scroll through menus and change settings.  Push the center of the control knob one or more times to select or change a setting.  Your Uconnect system may also have SCREEN OFF and MUTE buttons on the faceplate.  Push the SCREEN OFF button on the faceplate to turn off the Uconnect screen.  Push the button again or tap the screen to turn the screen on.  Press the Back Arrow button to exit out of a Menu or certain option on the Uconnect system.'\n",
      " ' UCONNECT SETTINGS The Uconnect system uses a combination of buttons on the touchscreen and buttons on the faceplate located on the center of the instrument panel.  These buttons allow you to access and change the Customer Programmable Features.  Many features can vary by vehicle.  Buttons on the faceplate are located below and/or beside the Uconnect system in the center of the instrument panel.  In addition, there is a SCROLL/ENTER control knob located on the right side.  Turn the control knob to scroll through menus and change settings.  Push the center of the control knob one or more times to select or change a setting.  Your Uconnect system may also have SCREEN OFF and MUTE buttons on the faceplate.  Push the SCREEN OFF button on the faceplate to turn off the Uconnect screen.  Push the button again or tap the screen to turn the screen on.  Press the Back Arrow button to exit out of a Menu or certain option on the Uconnect system.'] \n",
      "\n",
      "\n",
      "how to customize Uconnect system based on my own preferences? \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(answer, \"\\n\\n\")\n",
    "logger.info(documents,  \"\\n\\n\")\n",
    "logger.info(question, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-17 01:07:11 config.py:1865] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-17 01:07:38 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "WARNING 02-17 01:07:38 config.py:503] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 02-17 01:07:38 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 02-17 01:07:39 selector.py:135] Using Flash Attention backend.\n",
      "INFO 02-17 01:07:40 model_runner.py:1072] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 02-17 01:07:40 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 02-17 01:07:40 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5c5bf8802f4d6f899c3a8dfc5b6c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-17 01:07:42 model_runner.py:1077] Loading model weights took 2.8875 GB\n",
      "INFO 02-17 01:07:43 worker.py:232] Memory profiling results: total_gpu_memory=79.15GiB initial_memory_usage=3.52GiB peak_torch_memory=4.91GiB memory_usage_post_profile=3.54GiB non_torch_memory=0.64GiB kv_cache_size=69.64GiB gpu_memory_utilization=0.95\n",
      "INFO 02-17 01:07:43 gpu_executor.py:113] # GPU blocks: 162993, # CPU blocks: 9362\n",
      "INFO 02-17 01:07:43 gpu_executor.py:117] Maximum concurrency for 32768 tokens per request: 79.59x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.04s/it, est. speed input: 2.98 toks/s, output: 81.59 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM optimized inference time: 5.04 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qwen_client = HFClientVLLM(\n",
    "    model_path=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    ")\n",
    "\n",
    "# Basic generation\n",
    "response = qwen_client.generate(\"Explain quantum computing simply\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-17 01:10:33 config.py:1865] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-17 01:10:53 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "WARNING 02-17 01:10:53 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-17 01:10:53 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "WARNING 02-17 01:10:53 config.py:503] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 02-17 01:10:53 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 02-17 01:10:55 selector.py:135] Using Flash Attention backend.\n",
      "INFO 02-17 01:10:55 model_runner.py:1072] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "INFO 02-17 01:10:56 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 02-17 01:10:56 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18798875dd084570806f6635a4865aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-17 01:10:58 model_runner.py:1077] Loading model weights took 3.3460 GB\n",
      "INFO 02-17 01:10:58 worker.py:232] Memory profiling results: total_gpu_memory=79.15GiB initial_memory_usage=3.97GiB peak_torch_memory=4.73GiB memory_usage_post_profile=4.01GiB non_torch_memory=0.65GiB kv_cache_size=69.81GiB gpu_memory_utilization=0.95\n",
      "INFO 02-17 01:10:59 gpu_executor.py:113] # GPU blocks: 163388, # CPU blocks: 9362\n",
      "INFO 02-17 01:10:59 gpu_executor.py:117] Maximum concurrency for 131072 tokens per request: 19.94x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.03s/it, est. speed input: 562.22 toks/s, output: 80.32 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM optimized inference time: 2.04 seconds\n",
      "Raw LLM response: ```json\n",
      "{\n",
      "  \"learning_facilitation_score\": 0.75,\n",
      "  \"educational_strengths\": [\"Clear explanations\", \"Good examples\"],\n",
      "  \"areas_for_improvement\": [\"More details on varying feature settings\", \"Visual aids to aid understanding\"],\n",
      "  \"confidence\": 0.78\n",
      "}\n",
      "```\n",
      "{'learning_facilitation_score': 0.75, 'educational_strengths': ['Clear explanations', 'Good examples'], 'areas_for_improvement': ['More details on varying feature settings', 'Visual aids to aid understanding'], 'confidence': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "evaluator = LearningFacilitationEvaluator(\n",
    "    llm=HFClientVLLM(\n",
    "        model_path=model,\n",
    "    ),\n",
    "    prompt_manager=PromptManager(default_type=EvaluationType.LEARNING_FACILITATION)\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(\n",
    "    question=question,\n",
    "    context=documents,\n",
    "    answer=answer,\n",
    ")\n",
    "logger.info(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-17 01:13:40 config.py:1865] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 02-17 01:13:50 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 02-17 01:13:50 config.py:503] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 02-17 01:13:50 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 02-17 01:13:51 selector.py:135] Using Flash Attention backend.\n",
      "INFO 02-17 01:13:52 model_runner.py:1072] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
      "INFO 02-17 01:13:52 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7fbb0676a64bc9a40833c025b5a232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af010f96ea54c59a44489e26450db1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c66a95fe78340559a3f41cdaeb11787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b29bf38fe24cdb9b02eb737e265678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-17 01:15:57 model_runner.py:1077] Loading model weights took 6.0160 GB\n",
      "INFO 02-17 01:15:57 worker.py:232] Memory profiling results: total_gpu_memory=79.15GiB initial_memory_usage=6.54GiB peak_torch_memory=7.21GiB memory_usage_post_profile=6.56GiB non_torch_memory=0.54GiB kv_cache_size=67.45GiB gpu_memory_utilization=0.95\n",
      "INFO 02-17 01:15:58 gpu_executor.py:113] # GPU blocks: 39465, # CPU blocks: 2340\n",
      "INFO 02-17 01:15:58 gpu_executor.py:117] Maximum concurrency for 4096 tokens per request: 154.16x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.36s/it, est. speed input: 92.16 toks/s, output: 80.91 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM optimized inference time: 12.37 seconds\n",
      "Raw LLM response: ```json\n",
      "{\"learning_facilitation_score\": 0.8, \"educational_strengths\": [\"Clear explanations\", \"Step-by-step process\"], \"areas_for_improvement\": [\"Lack of visual aids\", \"No encouragement for further inquiry\"], \"confidence\": 0.88}\n",
      "``` \n",
      "\n",
      "Note: The score is based on the answer's ability to facilitate learning and education, considering the provided context. The strengths and areas for improvement are specific to the given answer. \n",
      "\n",
      "The answer provides clear explanations of the customization process and a step-by-step process to follow. However, it lacks visual aids and encourages further inquiry. To improve, the answer could benefit from the addition of diagrams or screenshots to illustrate the process and provide more guidance on how to access the settings. Additionally, the answer could encourage users to explore the Uconnect system further by asking questions or providing additional resources. \n",
      "\n",
      "The learning facilitation score of 0.8 indicates that the answer is generally effective in facilitating learning, but there is room for improvement. The confidence score of 0.88 suggests that the assistant is relatively confident in the answer's effectiveness. \n",
      "\n",
      "The educational strengths include clear explanations and a step-by-step process, which are essential for understanding the customization process. The areas for improvement include the lack of visual aids and the lack of encouragement for further inquiry. \n",
      "\n",
      "To address these areas for improvement, the assistant could revise the answer to include more visual aids, such as diagrams or screenshots, and encourage users to explore the Uconnect system further. This could be achieved by asking questions or providing additional resources, such as links to user manuals or online tutorials. \n",
      "\n",
      "By revising the answer in this way, the assistant can provide a more comprehensive and engaging learning experience for users, ultimately improving the learning facilitation score. \n",
      "\n",
      "Note: The revised answer should include visual aids, and the encouragement for further inquiry should be more explicit. For example:\n",
      "\n",
      "\"To customize the Uconnect system based on your own preferences, follow these steps:\n",
      "\n",
      "1. Press the apps button on the touchscreen to open the app screen.\n",
      "2. Press and hold the selected app, then drag it to replace an existing shortcut in the main menu bar.\n",
      "3. Note that this feature is only available when the vehicle is in PARK.\n",
      "\n",
      "For more information on the Uconnect system, please refer to the user manual or visit our website at [insert link]. You can also contact our customer support team for assistance.\n",
      "\n",
      "By following these steps and exploring the Uconnect system further, you can customize your driving experience to suit your preferences. Happy driving!\" \n",
      "\n",
      "This revised answer includes visual aids (the steps) and encourages users to explore the Uconnect system further by providing additional resources and asking questions. This can help to improve the learning facilitation score and provide a more engaging learning experience for users. \n",
      "\n",
      "However, it's worth noting that the revised answer is not significantly different from the original answer, and the improvements are relatively minor. A more significant revision would involve adding more visual aids, such as diagrams or screenshots, to illustrate the customization process and provide a more comprehensive understanding of the Uconnect system. \n",
      "\n",
      "In this case, the revised answer would receive a higher learning facilitation score, such as 0.9, indicating that it is more effective in facilitating learning and education. \n",
      "\n",
      "Note: The revised answer should include more visual aids and a more explicit encouragement for further inquiry. \n",
      "\n",
      "```json\n",
      "{\"learning_facilitation_score\": 0.9, \"educational_strengths\": [\"Clear explanations\", \"Step-by-step process\", \"Visual aids\"], \"areas_for_improvement\": [\"None\"], \"confidence\": 0.95}\n",
      "``` \n",
      "\n",
      "The revised answer is more effective in facilitating learning and education, with a higher learning facilitation score of 0.9. The educational strengths include clear explanations, a step-by-step process, and visual aids, which provide a comprehensive understanding of the customization process. The areas for improvement are nonexistent, indicating that the revised answer is effective in facilitating learning. The confidence score of 0.95 suggests that the assistant is highly confident in the revised answer's effectiveness. \n",
      "\n",
      "The revised answer is more engaging and comprehensive, providing a better learning experience for users. The addition of visual aids and explicit encouragement for further inquiry helps to improve the learning facilitation score and provides a more effective learning experience. \n",
      "\n",
      "Note: The revised answer should be significantly different from the original answer, with more visual aids and a more explicit encouragement for further inquiry. \n",
      "\n",
      "```json\n",
      "{\"learning_facilitation_score\": 0.95, \"educational_strengths\": [\"Clear explanations\", \"Step-by-step process\", \"Visual aids\", \"Interactive elements\"], \"areas_for_improvement\": [\"None\"], \"confidence\": 0.98}\n",
      "``` \n",
      "\n",
      "The revised answer is even more effective in facilitating learning and education, with a higher learning facilitation score of 0.95. The educational strengths include clear explanations, a step-by-step process\n",
      "Error parsing LLM response: \n",
      "{\"learning_facilitation_score\": 0.8, \"educational_strengths\": [\"Clear explanations\", \"Step-by-step process\"], \"areas_for_improvement\": [\"Lack of visual aids\", \"No encouragement for further inquiry\"], \"confidence\": 0.88}\n",
      " \n",
      "\n",
      "Note: The score is based on the answer's ability to facilitate learning and education, considering the provided context. The strengths and areas for improvement are specific to the given answer. \n",
      "\n",
      "The answer provides clear explanations of the customization process and a step-by-step process to follow. However, it lacks visual aids and encourages further inquiry. To improve, the answer could benefit from the addition of diagrams or screenshots to illustrate the process and provide more guidance on how to access the settings. Additionally, the answer could encourage users to explore the Uconnect system further by asking questions or providing additional resources. \n",
      "\n",
      "The learning facilitation score of 0.8 indicates that the answer is generally effective in facilitating learning, but there is room for improvement. The confidence score of 0.88 suggests that the assistant is relatively confident in the answer's effectiveness. \n",
      "\n",
      "The educational strengths include clear explanations and a step-by-step process, which are essential for understanding the customization process. The areas for improvement include the lack of visual aids and the lack of encouragement for further inquiry. \n",
      "\n",
      "To address these areas for improvement, the assistant could revise the answer to include more visual aids, such as diagrams or screenshots, and encourage users to explore the Uconnect system further. This could be achieved by asking questions or providing additional resources, such as links to user manuals or online tutorials. \n",
      "\n",
      "By revising the answer in this way, the assistant can provide a more comprehensive and engaging learning experience for users, ultimately improving the learning facilitation score. \n",
      "\n",
      "Note: The revised answer should include visual aids, and the encouragement for further inquiry should be more explicit. For example:\n",
      "\n",
      "\"To customize the Uconnect system based on your own preferences, follow these steps:\n",
      "\n",
      "1. Press the apps button on the touchscreen to open the app screen.\n",
      "2. Press and hold the selected app, then drag it to replace an existing shortcut in the main menu bar.\n",
      "3. Note that this feature is only available when the vehicle is in PARK.\n",
      "\n",
      "For more information on the Uconnect system, please refer to the user manual or visit our website at [insert link]. You can also contact our customer support team for assistance.\n",
      "\n",
      "By following these steps and exploring the Uconnect system further, you can customize your driving experience to suit your preferences. Happy driving!\" \n",
      "\n",
      "This revised answer includes visual aids (the steps) and encourages users to explore the Uconnect system further by providing additional resources and asking questions. This can help to improve the learning facilitation score and provide a more engaging learning experience for users. \n",
      "\n",
      "However, it's worth noting that the revised answer is not significantly different from the original answer, and the improvements are relatively minor. A more significant revision would involve adding more visual aids, such as diagrams or screenshots, to illustrate the customization process and provide a more comprehensive understanding of the Uconnect system. \n",
      "\n",
      "In this case, the revised answer would receive a higher learning facilitation score, such as 0.9, indicating that it is more effective in facilitating learning and education. \n",
      "\n",
      "Note: The revised answer should include more visual aids and a more explicit encouragement for further inquiry. \n",
      "\n",
      "\n",
      "{\"learning_facilitation_score\": 0.9, \"educational_strengths\": [\"Clear explanations\", \"Step-by-step process\", \"Visual aids\"], \"areas_for_improvement\": [\"None\"], \"confidence\": 0.95}\n",
      " \n",
      "\n",
      "The revised answer is more effective in facilitating learning and education, with a higher learning facilitation score of 0.9. The educational strengths include clear explanations, a step-by-step process, and visual aids, which provide a comprehensive understanding of the customization process. The areas for improvement are nonexistent, indicating that the revised answer is effective in facilitating learning. The confidence score of 0.95 suggests that the assistant is highly confident in the revised answer's effectiveness. \n",
      "\n",
      "The revised answer is more engaging and comprehensive, providing a better learning experience for users. The addition of visual aids and explicit encouragement for further inquiry helps to improve the learning facilitation score and provides a more effective learning experience. \n",
      "\n",
      "Note: The revised answer should be significantly different from the original answer, with more visual aids and a more explicit encouragement for further inquiry. \n",
      "\n",
      "\n",
      "{\"learning_facilitation_score\": 0.95, \"educational_strengths\": [\"Clear explanations\", \"Step-by-step process\", \"Visual aids\", \"Interactive elements\"], \"areas_for_improvement\": [\"None\"], \"confidence\": 0.98}\n",
      " \n",
      "\n",
      "The revised answer is even more effective in facilitating learning and education, with a higher learning facilitation score of 0.95. The educational strengths include clear explanations, a step-by-step process\n",
      "{'learning_facilitation_score': -1, 'educational_strengths': [], 'areas_for_improvement': [], 'confidence': -1, 'error': 'Extra data: line 5 column 1 (char 224)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model=\"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "evaluator = LearningFacilitationEvaluator(\n",
    "    llm=HFClientVLLM(\n",
    "        model_path=model,\n",
    "    ),\n",
    "    prompt_manager=PromptManager(default_type=EvaluationType.LEARNING_FACILITATION)\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate(\n",
    "    question=question,\n",
    "    context=documents,\n",
    "    answer=answer,\n",
    ")\n",
    "logger.info(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
