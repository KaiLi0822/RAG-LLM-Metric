\begin{thebibliography}{00}
\bibitem{b1} M. Moradi, K. Yan, D. Colwell, M. Samwald, and R. Asgari, “Exploring the landscape of large language models: Foundations, techniques, and challenges,” arXiv.org, 2024. \url{ https://arxiv.org/abs/2404.11973}

\bibitem{b2}W. Yuan, G. Neubig, and P. Liu, “BARTScore: Evaluating Generated Text as Text Generation,” arXiv:2106.11520 [cs], Oct. 2021, Available: \url{https://arxiv.org/abs/2106.11520}

\bibitem{b3} Y. Wang, A. Garcia Hernandez, R. Kyslyi, and N. Kersting, “Evaluating Quality of Answers for Retrieval-Augmented Generation: A Strong LLM Is All You Need.” Available: \url{https://arxiv.org/pdf/2406.18064}

\bibitem{b4} H. Yu, A. Gan, K. Zhang, S. Tong, Q. Liu, and Z. Liu, “Evaluation of Retrieval-Augmented Generation: A Survey.” Available: \url{https://arxiv.org/pdf/2405.07437}

\bibitem{b5} P. Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” arXiv.org, Apr. 12, 2021. \url{https://arxiv.org/abs/2005.11401}

\bibitem{b6} H. Wu, Z. Li, J. Zhao, Z. Liu, Y. Bai, and J. Tang, “Agentic retrieval-augmented generation: A survey on agentic RAG,” arXiv.org, 2025. Available: \url{https://arxiv.org/abs/2501.09136}

\bibitem{b7} R. Friel, M. Belyi, and A. Sanyal, “RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems,” arXiv.org, 2024. Available: \url{https://arxiv.org/abs/2407.11005}

\bibitem{b8} S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “RAGAS: Automated Evaluation of Retrieval Augmented Generation,” arXiv (Cornell University), Sep. 2023, doi: \url{https://doi.org/10.48550/arxiv.2309.15217}

\bibitem{b9} P. Verga et al., “Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models,” arXiv.org, May 01, 2024. \url{https://arxiv.org/abs/2404.18796}

\bibitem{b10} A. Elangovan et al., “Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge,” arXiv.org, 2024. \url{https://arxiv.org/abs/2410.03775}

\bibitem{b11} B. Malin, T. Kalganova, and N. Boulgouris, "A Review of Faithfulness Metrics for Hallucination Assessment in Large Language Models," arXiv prelogger.info arXiv, 2024. Available: \url{https://arxiv.org/abs/2501.00269}

\bibitem{b12} C.-M. Chan et al., “ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,” arXiv.org, Aug. 14, 2023. \url{https://arxiv.org/abs/2308.07201}

\bibitem{b13} Y. Yu et al., “Self-Generated Critiques Boost Reward Modeling for Language Models,” arXiv.org, 2024. \url{https://arxiv.org/abs/2411.16646}

\bibitem{b14} Matteo Gabburo, S. Garg, Rik Koncel-Kedziorski, and Alessandro Moschitti, “SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References,” arXiv (Cornell University), pp. 20–28, Jan. 2023, doi: \url{https://doi.org/10.18653/v1/2023.ijcnlp-short.3}

\bibitem{b15} W. Xu et al., “INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback,” arXiv.org, 2023. \url{https://arxiv.org/abs/2305.14282}

\bibitem{b16} K. Papineni, S. Roukos, T. Ward, and W. J. Zhu, “BLEU: A method for automatic evaluation of machine translation,” in Proc. 40th Annu. Meet. Assoc. Comput. Linguist., 2002, pp. 311–318. \url{https://doi.org/10.3115/1073083.1073135}

\bibitem{b17} C. Y. Lin, “ROUGE: A package for automatic evaluation of summaries,” in Workshop on Text Summarization Branches Out (WAS 2004), 2004. Available: \url{https://aclanthology.org/W04-1013/}

\bibitem{b18} S. Roychowdhury, S. Soman, R. H. G, N. Gunda, V. Chhabra, and S. K. Bala, “Evaluation of RAG Metrics for Question Answering in the Telecom Domain,” arXiv.org, 2024. \url{https://arxiv.org/abs/2407.12873}

\bibitem{b19} M. Chowdhury, Y. V. He, A. Higham, and E. Lim, "ASTRID: An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems," arXiv prelogger.info arXiv:2501.08208, 2025.  Available: \url{https://arxiv.org/abs/2501.08208}

\bibitem{b20} J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking Large Language Models in Retrieval-Augmented Generation,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 16, pp. 17754–17762, Mar. 2024, doi: \url{https://doi.org/10.1609/aaai.v38i16.29728}

\bibitem{b21} K. Zhu et al., “RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework,” arXiv.org, 2024. \url{https://arxiv.org/abs/2408.01262}

\bibitem{b22} G. Guinet, B. Omidvar-Tehrani, A. Deoras, and L. Callot, “Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation,” arXiv.org, 2024. \url{https://arxiv.org/abs/2405.13622}

\bibitem{b23} I. Papadimitriou, I. Gialampoukidis, S. Vrochidis, and I. Kompatsiaris, "RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems," arXiv prelogger.info arXiv:2412.12322, 2024. Available: \url{https://arxiv.org/abs/2412.12322}

\bibitem{b24} Z. Jin, H. Yuan, T. Men, P. Cao, Y. Chen, K. Liu, and J. Zhao, "RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment," arXiv prelogger.info arXiv:2412.13746, 2024. Available: \url{https://arxiv.org/abs/2412.13746}

\bibitem{b25} Farea, Amer, et al. Evaluation of Question Answering Systems: Complexity of Judging a Natural Language. arXiv:2209.12617, arXiv, 10 Sept. 2022. arXiv.org, \url{https://doi.org/10.48550/arXiv.2209.12617}

\bibitem{b26} Trulens, 2023. \url{https://www.trulens.org/}

\bibitem{b27} A. AI, “RAGEval: Scenario-Specific RAG Evaluation Dataset Generation Framework,” Athina AI Hub, Aug. 27, 2024. \url{https://hub.athina.ai/research-papers/rageval-scenario-specific-rag-evaluation-dataset-generation-framework/}

\bibitem{b28} S. Wang, J. Tan, Z. Dou, and J.-R. Wen, “OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain,” arXiv.org, 2024. \url{https://arxiv.org/abs/2412.13018}

\bibitem{b29} X. Dai, S. Karimi, and B. Fang, “A Critical Look at Meta-evaluating Summarisation Evaluation Metrics,” arXiv.org, 2024. \url{https://arxiv.org/abs/2409.19507}

\bibitem{b30} W. Xu, D. Wang, L. Pan, Z. Song, M. Freitag, W. Y. Wang, and L. Li, “InstructScore: Explainable text generation evaluation with fine-grained feedback,” arXiv:2305.14282 [cs.CL], 2023. \url{ https://arxiv.org/abs/2305.14282}

\end{thebibliography}